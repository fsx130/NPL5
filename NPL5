#Bert模型
from transformers import BertTokenizer
import torch
from transformers import BertForSequenceClassification
import torch.optim as optim

#文本分类
# 加载 BERT tokenizer 和模型
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertForSequenceClassification.from_pretrained('bert-base-uncased')
# 准备数据
sentences = ["太好了", "太坏了"]
labels = [0, 1]
encodings = tokenizer(sentences, truncation=True, padding=True)
# 将数据批量转换为tensor
input_ids = torch.tensor(encodings['input_ids'])
attention_mask = torch.tensor(encodings['attention_mask'])
labels = torch.tensor(labels)
# 配置优化器和学习率
optimizer = optim.AdamW(model.parameters(), lr=1e-5)
# 训练和微调BERT模型
model.train()
for epoch in range(10):
    outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
    loss, logits = outputs[:2]
    loss.backward()
    optimizer.step()
    optimizer.zero_grad()

# 预测文本标签
model.eval()
with torch.no_grad():
    # 添加多个测试句子
    test_sentences = ["这家餐馆的食物真是太好吃了", "这部电影真棒", "这辆车很难开", "这本书太无聊了", "这首歌非常好听"]
    test_labels = torch.tensor([0, 0, 1, 1, 0])  # 前两个是正面的，后三个是负面的

    # 编码测试数据
    encodings = tokenizer(test_sentences, truncation=True, padding=True)
    input_ids = torch.tensor(encodings['input_ids'])
    attention_mask = torch.tensor(encodings['attention_mask'])

    # 做出预测并计算准确率
    outputs = model(input_ids, attention_mask=attention_mask)
    predicted_labels = torch.argmax(outputs.logits, dim=1)
    # print(f"test_sentences: {test_sentences}  Predicted sentiment: {predicted_labels}")
    accuracy = (predicted_labels == test_labels).float().mean().item()
    print(f"Accuracy: {accuracy}")



# 生成摘要
import torch
from transformers import BertTokenizer, BertForMaskedLM
from rouge import Rouge

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertForMaskedLM.from_pretrained('bert-base-uncased')
model.eval()

input_text = "The method of machine learning effectively utilizes the powerful computational performance of computers, utilizing statistical knowledge theory to efficiently and reasonably model massive amounts of text information, and can discover hidden attributes hidden in massive amounts of text information"
# 添加特殊标记
input_text = '[CLS]' + input_text + ' [MASK] [SEP]'

# 将输入文本编码为ID序列
input_ids = tokenizer.encode(input_text, add_special_tokens=True)
mask_index = input_ids.index(tokenizer.mask_token_id)
input_ids = torch.tensor([input_ids])
beam_output = model.generate(input_ids=input_ids, max_length=50, num_beams=8, no_repeat_ngram_size=2, early_stopping=True)
output_text = tokenizer.decode(beam_output[0], skip_special_tokens=True)
print(output_text)
# 使用ROUGE指标评估效果
rouge = Rouge()
scores = rouge.get_scores(output_text, input_text)
print(scores)


# 问答
import torch
from transformers import BertTokenizer, BertForQuestionAnswering
# 加载 BERT tokenizer 和 BERT QA 模型
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertForQuestionAnswering.from_pretrained('bert-base-uncased')
# 将模型移动到GPU上或者CPU上
device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
model.to(device)
# 定义 answer_question 函数
def answer_question(question, text):
    # 对问题和文本进行编码
    inputs = tokenizer.encode_plus(question, text, add_special_tokens=True, return_tensors='pt')
    inputs = {k: v.to(device) for k, v in inputs.items()}
    # 使用 BERT 模型进行问题答案预测
    outputs = model(**inputs)
    start_scores, end_scores = outputs.start_logits, outputs.end_logits
    # 获取预测的起始位置和结束位置
    answer_start = torch.argmax(start_scores)
    answer_end = torch.argmax(end_scores) + 1
    # 将起始位置和结束位置转换为文本中的词片段
    answer_tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0][answer_start:answer_end])
    answer = ' '.join(answer_tokens).replace('[SEP]', '').replace(' ##', '').replace('[CLS]', '').replace('[UNK]', '').strip()

    return answer
# 测试函数
text = "紫禁城又称故宫，是中国北京市中心的一组古建筑群，是明清两代的皇宫。"
question = "紫禁城是什么？"
answer = answer_question(question, text)
print(answer)

question1 = "What is the capital of France?"
text1 = "France is a country in Europe. Its capital is Paris and its population is over 66 million."
answer1 = answer_question(question1, text1)
print(answer1)



# 翻译
import nltk
nltk.download('punkt')
from transformers import BertTokenizer, EncoderDecoderModel

# 加载BertTokenizer和EncoderDecoderModel
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = EncoderDecoderModel.from_encoder_decoder_pretrained('bert-base-uncased', 'bert-base-uncased')

# 定义源语言和目标语言的文本
source_text = "你好，你今天怎么样"
target_text = "Hello, how are you today"

# 对源语言进行编码，并添加特殊令牌
inputs = tokenizer(source_text, return_tensors="pt", padding=True, truncation=True, max_length=128) # 对输入进行了padding和截断处理
input_ids = inputs["input_ids"]
attention_mask = inputs["attention_mask"]

# 得到解码后的序列长度
decoder_input_ids = tokenizer(target_text, return_tensors='pt', padding=True, truncation=True, max_length=128)['input_ids'][:, :-1]
outputs = model.generate(
    input_ids,
    attention_mask=attention_mask,
    decoder_input_ids=decoder_input_ids,
    max_length=len(decoder_input_ids[0])+5, # 控制最大生成长度
    decoder_start_token_id=model.config.decoder.pad_token_id
)
out_text = tokenizer.decode(outputs[0], skip_special_tokens=True) # batch_decode转为decode[0]避免字符串报错
print(f"Source text: {source_text}")
print(f"Translated text: {out_text}")

# # 计算BLEU得分
# reference_tokens = nltk.word_tokenize(target_text.lower())
# output_tokens = nltk.word_tokenize(target_text.lower())
# bleu_score = nltk.translate.bleu_score.sentence_bleu([reference_tokens], output_tokens)
# print(f"BLEU score: {bleu_score:.2f}")

#T5模型
from transformers import T5ForConditionalGeneration, T5Tokenizer

# 加载 T5 tokenizer 和模型
tokenizer = T5Tokenizer.from_pretrained('t5-base')
model = T5ForConditionalGeneration.from_pretrained('t5-base')

# 标记好的数据集
data = {
    "这家餐馆的食物真是太好吃了": "positive",
    "这部电影真棒": "positive",
    "这辆车很难开": "negative",
    "这本书太无聊了": "negative",
    "这首歌非常好听": "positive"
}

# 将输入文本转换成模型可接受的格式
sentences = list(data.keys())
labels = list(data.values())
inputs = [f"sentence: {sent}" for sent in sentences]
input_ids = tokenizer.batch_encode_plus(inputs, return_tensors='pt', padding=True, truncation=True)['input_ids']

# 通过模型预测情感分类
outputs = model.generate(input_ids=input_ids, max_length=2)
predicted_labels = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]
predicted_labels = [0 if label == 'negative' else 1 for label in predicted_labels]

# 计算分类的准确率
correct = 0
for i in range(len(labels)):
    if labels[i] == "positive":
        correct += int(predicted_labels[i] == 1)
    else:
        correct += int(predicted_labels[i] == 0)
accuracy = correct / len(labels)

# 输出每个句子的情感分类结果和准确率
for sentence, label, pred in zip(sentences, labels, predicted_labels):
    print(f"Sentence: {sentence}  Actual sentiment: {label}  Predicted sentiment: {labels[pred]}")
print(f"Accuracy: {accuracy}")

from transformers import T5ForConditionalGeneration, T5Tokenizer
from rouge import Rouge
# 加载T5模型和分词器
model_name = "t5-small"
tokenizer = T5Tokenizer.from_pretrained(model_name)
model = T5ForConditionalGeneration.from_pretrained(model_name)

# 定义源文本
text = "The method of machine learning effectively utilizes the powerful computational performance of computers, utilizing statistical knowledge theory to efficiently and reasonably model massive amounts of text information, and can discover hidden attributes hidden in massive amounts of text information"

# 生成摘要
input_ids = tokenizer.encode(text, return_tensors="pt", max_length=512, truncation=True)
summary_ids = model.generate(input_ids, max_length=50, min_length=10, length_penalty=2.0, num_beams=4, early_stopping=True)
summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)

# 打印摘要
print("摘要：", summary)
rouge = Rouge()
scores = rouge.get_scores(summary, text)
print(scores)

# 问答
from transformers import T5ForConditionalGeneration, T5Tokenizer, pipeline

# 加载T5模型和分词器
model_name = 't5-base'
tokenizer = T5Tokenizer.from_pretrained(model_name, model_max_length=1024)
model = T5ForConditionalGeneration.from_pretrained(model_name)

# 定义问题和文本
question = "What is the capital of France?"
context = "France is a country in Europe. Its capital is Paris and its population is over 66 million."

question1 = "紫禁城是什么？"
context1 = "紫禁城又称故宫，是中国北京市中心的一组古建筑群，是明清两代的皇宫。"
# 构造输入，并编码
input_str = "question: " + question + " context: " + context
input_ids = tokenizer.encode(input_str, return_tensors='pt')
# 生成答案
answer = pipeline('text2text-generation', model=model, tokenizer=tokenizer, device=-1)
outputs = answer(input_str, max_length=64)
# 解码并打印答案
generated = outputs[0]['generated_text']
print(generated)

# 构造输入，并编码
input_str1 = "question1: " + question1 + " context1: " + context1
input_ids1 = tokenizer.encode(input_str1, return_tensors='pt')
# 生成答案
answer1 = pipeline('text2text-generation', model=model, tokenizer=tokenizer, device=-1)
outputs1 = answer1(input_str1, max_length=64)
# 解码并打印答案
generated1 = outputs1[0]['generated_text']
print(generated1)


#翻译
from transformers import T5ForConditionalGeneration, T5Tokenizer

# 加载T5模型和分词器
model_name = 't5-base'
max_length = 64
tokenizer = T5Tokenizer.from_pretrained(model_name, model_max_length=max_length)
model = T5ForConditionalGeneration.from_pretrained(model_name)

# 定义要翻译的内容
text = "你好，你今天怎么样？"
# 构造输入，并编码
input_text = f"translate Chinese to English: {text}"
input_ids = tokenizer.encode(input_text, return_tensors='pt', max_length=max_length, truncation=True)

# 生成翻译
outputs = model.generate(input_ids, max_length=max_length, num_beams=4, early_stopping=True)
# 解码并打印翻译结果
generated = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(f"Source text: {text}")
print(f"Translated text: {generated}")


#GPT-2模型
# import torch
# from transformers import GPT2Tokenizer, GPT2ForSequenceClassification
#
# # 加载 GPT-2 tokenizer 和模型
# tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
# tokenizer.add_special_tokens({'pad_token': '[PAD]'})
# model = GPT2ForSequenceClassification.from_pretrained('gpt2')
#
#
# # 设定情感类别标签和对应的 ID
# labels = ['negative', 'positive']
# label_map = {label: i for i, label in enumerate(labels)}
#
# # 定义用于计算正确率的变量
# num_correct = 0
# num_total = 0
#
# # 定义每个句子的真实标签
# true_labels = ['positive', 'positive', 'negative', 'negative', 'positive']
# sentences = ["这家餐馆的食物真是太好吃了", "这部电影真棒", "这辆车很难开", "这本书太无聊了", "这首歌非常好听"]
# # 对每个句子进行情感分类
# for sentence, true_label in zip(sentences, true_labels):
#     # 使用 tokenizer 将句子转换为模型可接受的格式
#     inputs = tokenizer(sentence, return_tensors='pt', padding=True, truncation=True)
#     # 使用模型进行预测，并获取输出的概率向量
#     outputs = model(**inputs)
#     logits = outputs.logits
#     probabilities = torch.softmax(logits, dim=-1)
#     # 找到最大概率的标签，并输出情感分类结果
#     predicted_label_id = torch.argmax(probabilities[0]).item()
#     predicted_label = labels[predicted_label_id]
#     print(f"Sentence: {sentence}  Predicted sentiment: {predicted_label}")
#     # 根据预测结果计算正确率
#     true_label_id = label_map[true_label]
#     if predicted_label_id == true_label_id:
#         num_correct += 1
#     num_total += 1
#
# # 输出正确率
# accuracy = num_correct / num_total
# print(f"Accuracy: {accuracy}")

# #问答
# from transformers import pipeline, AutoTokenizer, AutoModelForQuestionAnswering
#
# # 加载模型和分词器
# model_name = 'distilgpt2'
# tokenizer = AutoTokenizer.from_pretrained(model_name)
# model = AutoModelForQuestionAnswering.from_pretrained(model_name)
#
# # 定义问题和文本
# question = "What is the capital of France?"
# text = "France is a country in Europe. Its capital is Paris and its population is over 66 million."
#
# question1 = "紫禁城是什么？"
# text1 = "紫禁城又称故宫，是中国北京市中心的一组古建筑群，是明清两代的皇宫。"
#
# # 对问题和文本进行编码和分批处理
# inputs = tokenizer(question, text, add_special_tokens=True, return_tensors="pt")
# inputs1 = tokenizer(question1, text1, add_special_tokens=True, return_tensors="pt")
#
# # 使用模型生成答案
# qa_pipeline = pipeline('question-answering', model=model, tokenizer=tokenizer)
# answer = qa_pipeline(question=question, context=text)
#
# qa_pipeline1 = pipeline('question-answering', model=model, tokenizer=tokenizer)
# answer1 = qa_pipeline1(question=question1, context=text1)
#
# # 打印答案
# print("答案：", answer['answer'])
# print("答案1：", answer1['answer'])

# 产生摘要
from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM
from rouge import Rouge

# # 加载GPT-2模型和分词器
# model_name = "gpt2"
# tokenizer = AutoTokenizer.from_pretrained(model_name)
# model = AutoModelForCausalLM.from_pretrained(model_name)
#
# # 加载摘要生成器
# summarizer = pipeline("summarization", model=model, tokenizer=tokenizer)
#
# # 定义源文本
# text = "The method of machine learning effectively utilizes the powerful computational performance of computers, utilizing statistical knowledge theory to efficiently and reasonably model massive amounts of text information, and can discover hidden attributes hidden in massive amounts of text information"
#
# # 生成摘要
# summary = summarizer(text, max_length=50, min_length=10, do_sample=False)[0]["summary_text"]
#
# # 打印摘要
# print("摘要：", summary)
#
# # 使用ROUGE指标评估效果
# rouge = Rouge()
# scores = rouge.get_scores(summary, text)
# print(scores)

# 翻译
from transformers import AutoTokenizer, pipeline
from nltk.translate.bleu_score import corpus_bleu

# 加载模型和分词器
model_name = "Helsinki-NLP/opus-mt-zh-en"
tokenizer = AutoTokenizer.from_pretrained(model_name)
translator = pipeline("translation", model=model_name, tokenizer=tokenizer)

# 定义中文文本
text = "你好，你今天怎么样"

# 定义参考句子，用于计算BLEU得分
references = [["Hello, how are you today"]]

# 翻译中文到英文
translation = translator(text, max_length=512)

# 打印翻译结果
print(f"Source text: {text}")
print("Translated text:：", translation[0]["translation_text"])

# 计算BLEU得分
hypotheses = [translation[0]["translation_text"]]
bleu_score = corpus_bleu(references, hypotheses)
# 打印BLEU得分
print("BLEU score：", bleu_score)
